{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "datasets = []\n",
    "\n",
    "random_graph_directory = \"./graphs/\"\n",
    "for root, dirs, files in os.walk(random_graph_directory):\n",
    "    for filename in files:\n",
    "        name_string = filename[:-4]\n",
    "        graph_file_path = os.path.join(root, filename)\n",
    "\n",
    "        G = nx.read_gml(graph_file_path)\n",
    "\n",
    "        datasets.append(\n",
    "            {\n",
    "                \"name\": name_string,\n",
    "                \"graph\": nx.relabel.convert_node_labels_to_integers(\n",
    "                    G, first_label=0\n",
    "                ),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Solver Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solvers.dNNMIS_GPU_TAU import DNNMIS as V2\n",
    "from solvers.KaMIS import ReduMIS\n",
    "from solvers.ILPMIS import ILPMIS\n",
    "\n",
    "solvers = [\n",
    "    # {\n",
    "    #     \"name\": \"dNN V2 GPU\",\n",
    "    #     \"class\": V2,\n",
    "    #     \"params\": {\n",
    "    #         \"learning_rate\": 0.05,\n",
    "    #         \"selection_criteria\": 0.5,\n",
    "    #         \"max_steps\": 150000\n",
    "    #     },\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"dNN\",\n",
    "    #     \"class\": DNNMIS,\n",
    "    #     \"params\": {\n",
    "    #         \"learning_rate\": 0.001,\n",
    "    #         \"selection_criteria\": 0.8,\n",
    "    #         \"max_steps\": 25000,\n",
    "    #     },\n",
    "    # },\n",
    "    #     {\n",
    "    #     \"name\": \"dNN w/SG5k\",\n",
    "    #     \"class\": DNNMIS,\n",
    "    #     \"params\": {\n",
    "    #         \"learning_rate\": 0.001,\n",
    "    #         \"selection_criteria\": 0.65,\n",
    "    #         \"max_steps\": 80000,\n",
    "    #         \"max_subgraph_steps\": 5000,\n",
    "    #     },\n",
    "    # },\n",
    "    {\"name\": \"ReduMIS\", \"class\": ReduMIS, \"params\": {\"seed\": 13}},\n",
    "    # {\"name\": \"ILP\", \"class\": ILPMIS, \"params\": {\"time_limit\": 935}}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Run New Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def benchmark(datasets, solvers):\n",
    "    solutions = []\n",
    "\n",
    "    stage = 0\n",
    "    stages = len(solvers) * len(datasets)\n",
    "\n",
    "    for solver in solvers:\n",
    "        for dataset in datasets:\n",
    "            solver_instance = solver[\"class\"](dataset[\"graph\"], solver[\"params\"])\n",
    "            solver_instance.solve()\n",
    "            solution = {\n",
    "                \"solution_method\": solver[\"name\"],\n",
    "                \"dataset_name\": dataset[\"name\"],\n",
    "                \"data\": deepcopy(solver_instance.solution),\n",
    "                \"time_taken\": deepcopy(solver_instance.solution_time),\n",
    "            }\n",
    "            solutions.append(solution)\n",
    "            del solver_instance\n",
    "            stage += 1\n",
    "            print(f\"Completed {stage} / {stages}\")\n",
    "\n",
    "    return solutions\n",
    "\n",
    "solutions = benchmark(datasets, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0]\n",
      "[3, 4, 7, 9, 13]\n",
      "[3, 4, 7, 9, 13]\n",
      "[1 0 0 0 1 0 0 0 0 1 1 0]\n",
      "[0, 4, 9, 10]\n",
      "[0, 4, 9, 10]\n",
      "[0 1 0 0 0 0 0 0 0 1 1 0 0 1]\n",
      "[1, 9, 10, 13]\n",
      "[1, 9, 10, 13]\n",
      "[1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0]\n",
      "[0, 4, 6, 11, 14]\n",
      "[0, 4, 6, 11, 14]\n",
      "[1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      "[0, 1, 2, 3, 12, 16]\n",
      "[0, 1, 2, 3, 12, 16]\n",
      "[0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0]\n",
      "[1, 4, 11, 13, 14]\n",
      "[1, 4, 11, 13, 14]\n",
      "[0 0 0 0 1 1 0 0 1 0 0 0 1 0]\n",
      "[4, 5, 8, 12]\n",
      "[4, 5, 8, 12]\n",
      "[0 0 1 0 0 1 0 0 0 1]\n",
      "[2, 5, 9]\n",
      "[2, 5, 9]\n",
      "[0 0 1 0 0 1 0 1 0 0 1 0 1 0]\n",
      "[2, 5, 7, 10, 12]\n",
      "[2, 5, 7, 10, 12]\n",
      "[1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "[0, 2, 3, 4, 17]\n",
      "[0, 2, 3, 4, 17]\n",
      "[0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "[1, 3, 4, 7, 13]\n",
      "[1, 3, 4, 7, 13]\n",
      "[0 0 0 0 1 0 0 0 1 0 1 1]\n",
      "[4, 8, 10, 11]\n",
      "[4, 8, 10, 11]\n",
      "[0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0]\n",
      "[1, 4, 10, 11, 13]\n",
      "[1, 4, 10, 11, 13]\n",
      "[0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0]\n",
      "[4, 6, 10, 11, 14]\n",
      "[4, 6, 10, 11, 14]\n",
      "[0 0 1 0 0 0 1 1 1 0]\n",
      "[2, 6, 7, 8]\n",
      "[2, 6, 7, 8]\n",
      "[0 1 0 1 1 0 0 0 0 0 1 0 1 0]\n",
      "[1, 3, 4, 10, 12]\n",
      "[1, 3, 4, 10, 12]\n",
      "[0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0]\n",
      "[3, 4, 12, 17, 18]\n",
      "[3, 4, 12, 17, 18]\n",
      "[0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1]\n",
      "[3, 5, 7, 15]\n",
      "[3, 5, 7, 15]\n",
      "[0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1]\n",
      "[2, 10, 13, 15, 17]\n",
      "[2, 10, 13, 15, 17]\n",
      "[0 0 1 0 0 0 0 1 0 1 0 1 0 1]\n",
      "[2, 7, 9, 11, 13]\n",
      "[2, 7, 9, 11, 13]\n",
      "[1 0 1 0 0 0 1 1 0 0 0 1 0 0]\n",
      "[0, 2, 6, 7, 11]\n",
      "[0, 2, 6, 7, 11]\n",
      "[0 0 1 0 1 1 1 0 0 0 0 0]\n",
      "[2, 4, 5, 6]\n",
      "[2, 4, 5, 6]\n",
      "[0 1 0 0 1 0 1 1 0 0]\n",
      "[1, 4, 6, 7]\n",
      "[1, 4, 6, 7]\n",
      "[1 0 1 0 0 0 1 1 0 0 0 1]\n",
      "[0, 2, 6, 7, 11]\n",
      "[0, 2, 6, 7, 11]\n",
      "[0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      "[1, 7, 8, 12]\n",
      "[1, 7, 8, 12]\n",
      "[1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0]\n",
      "[0, 1, 5, 11, 17]\n",
      "[0, 1, 5, 11, 17]\n",
      "[1 0 1 0 0 0 0 1 0 0 1 1]\n",
      "[0, 2, 7, 10, 11]\n",
      "[0, 2, 7, 10, 11]\n",
      "[0 0 1 0 1 1 0 1 0 0]\n",
      "[2, 4, 5, 7]\n",
      "[2, 4, 5, 7]\n",
      "[0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0]\n",
      "[1, 7, 9, 10, 16]\n",
      "[1, 7, 9, 10, 16]\n",
      "[1 0 1 0 0 1 0 0 1 1]\n",
      "[0, 2, 5, 8, 9]\n",
      "[0, 2, 5, 8, 9]\n",
      "[1 0 0 1 0 0 0 0 1 0]\n",
      "[0, 3, 8]\n",
      "[0, 3, 8]\n",
      "[1 0 1 0 1 0 0 0 0 0 0 1]\n",
      "[0, 2, 4, 11]\n",
      "[0, 2, 4, 11]\n",
      "[1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0]\n",
      "[0, 2, 4, 5, 14]\n",
      "[0, 2, 4, 5, 14]\n",
      "[0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0]\n",
      "[3, 4, 6, 13, 14]\n",
      "[3, 4, 6, 13, 14]\n",
      "[1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0]\n",
      "[0, 5, 6, 12, 13]\n",
      "[0, 5, 6, 12, 13]\n",
      "[0 0 0 0 1 0 1 1 0 0]\n",
      "[4, 6, 7]\n",
      "[4, 6, 7]\n",
      "[1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1]\n",
      "[0, 3, 4, 11, 13, 17]\n",
      "[0, 3, 4, 11, 13, 17]\n",
      "[0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0]\n",
      "[3, 5, 10, 13, 15, 16]\n",
      "[3, 5, 10, 13, 15, 16]\n",
      "[0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0]\n",
      "[4, 7, 9, 10, 12, 14]\n",
      "[4, 7, 9, 10, 12, 14]\n",
      "[1 0 1 0 0 0 0 0 0 1 1 0]\n",
      "[0, 2, 9, 10]\n",
      "[0, 2, 9, 10]\n",
      "[0 1 0 1 0 0 0 1 0 1 0 0 1 0]\n",
      "[1, 3, 7, 9, 12]\n",
      "[1, 3, 7, 9, 12]\n",
      "[0 0 0 0 0 0 1 0 1 1]\n",
      "[6, 8, 9]\n",
      "[6, 8, 9]\n",
      "[0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1]\n",
      "[8, 10, 11, 15]\n",
      "[8, 10, 11, 15]\n",
      "[1 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
      "[0, 5, 6, 9]\n",
      "[0, 5, 6, 9]\n",
      "[1 0 0 1 0 0 1 1 0 0]\n",
      "[0, 3, 6, 7]\n",
      "[0, 3, 6, 7]\n",
      "[0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1]\n",
      "[5, 6, 8, 10, 14, 17]\n",
      "[5, 6, 8, 10, 14, 17]\n",
      "[0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0]\n",
      "[3, 4, 7, 14, 16]\n",
      "[3, 4, 7, 14, 16]\n",
      "[1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0]\n",
      "[0, 3, 4, 11, 17]\n",
      "[0, 3, 4, 11, 17]\n",
      "[0 1 0 1 0 0 0 1 0 0 0 1]\n",
      "[1, 3, 7, 11]\n",
      "[1, 3, 7, 11]\n",
      "[0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0]\n",
      "[2, 5, 6, 12, 17]\n",
      "[2, 5, 6, 12, 17]\n",
      "[0 1 0 1 1 0 1 0 0 0]\n",
      "[1, 3, 4, 6]\n",
      "[1, 3, 4, 6]\n",
      "[0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0]\n",
      "[2, 5, 8, 11, 14]\n",
      "[2, 5, 8, 11, 14]\n",
      "[0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1]\n",
      "[1, 6, 13, 14, 15]\n",
      "[1, 6, 13, 14, 15]\n",
      "[1 0 0 0 1 1 0 0 1 0 1 0 0 0]\n",
      "[0, 4, 5, 8, 10]\n",
      "[0, 4, 5, 8, 10]\n",
      "[0 1 0 1 0 1 1 1 0 0 0 0]\n",
      "[1, 3, 5, 6, 7]\n",
      "[1, 3, 5, 6, 7]\n",
      "[0 1 0 0 0 0 1 0 0 0 1 1]\n",
      "[1, 6, 10, 11]\n",
      "[1, 6, 10, 11]\n",
      "[1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1]\n",
      "[0, 5, 6, 14, 17]\n",
      "[0, 5, 6, 14, 17]\n",
      "[0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1]\n",
      "[7, 8, 13, 14, 16, 19]\n",
      "[7, 8, 13, 14, 16, 19]\n",
      "[0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]\n",
      "[1, 10, 11, 12]\n",
      "[1, 10, 11, 12]\n",
      "[0 0 0 1 1 1 0 0 0 0 0 1 1 0]\n",
      "[3, 4, 5, 11, 12]\n",
      "[3, 4, 5, 11, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167791/549185145.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "dataset_index = {k: v for v, k in enumerate([dataset[\"name\"] for dataset in datasets])}\n",
    "datasets_solutions = [[] for i in range(len(datasets))]\n",
    "\n",
    "def MIS_checker(MIS_list, G):\n",
    "    pairs = list(combinations(MIS_list, 2))\n",
    "    IS_CHECKER = True\n",
    "    if len(MIS_list) > 1:\n",
    "        for pair in pairs:\n",
    "            if (pair[0], pair[1]) in G.edges or (pair[1], pair[0]) in G.edges:\n",
    "                IS_CHECKER = False\n",
    "                break\n",
    "    return IS_CHECKER\n",
    "\n",
    "table_data = []\n",
    "\n",
    "for solution in solutions:\n",
    "    dsi = dataset_index[solution[\"dataset_name\"]]\n",
    "    datasets_solutions[dsi].append(solution)\n",
    "\n",
    "i = 0\n",
    "for dataset_solutions in datasets_solutions:\n",
    "    # IS CHECK\n",
    "    is_check = []\n",
    "    for solution in dataset_solutions:\n",
    "        IS_set = solution[\"data\"][\"graph_mask\"]\n",
    "        print(IS_set)\n",
    "        indices = [i for i, x in enumerate(IS_set) if x == 1]\n",
    "        print(indices)\n",
    "        subgraph = datasets[dataset_index[solution[\"dataset_name\"]]][\"graph\"].subgraph(indices)\n",
    "        subgraph = nx.Graph(subgraph)\n",
    "        while len(subgraph) > 0:\n",
    "            degrees = dict(subgraph.degree())\n",
    "            max_degree_nodes = [node for node, degree in degrees.items() if degree == max(degrees.values())]\n",
    "            \n",
    "            if len(max_degree_nodes) == 0 or subgraph.degree(max_degree_nodes[0]) == 0:\n",
    "                break  # No more nodes to remove or all remaining nodes have degree 0\n",
    "\n",
    "            subgraph.remove_node(max_degree_nodes[0])\n",
    "        nodes = list(subgraph)\n",
    "        nodes.sort()\n",
    "        print(nodes)\n",
    "        # subgraph = datasets[dataset_index[solution[\"dataset_name\"]]][\"graph\"].subgraph(\n",
    "        #     IS_set\n",
    "        # )\n",
    "        # if len(subgraph.edges) > 0:\n",
    "        #     plt.figure(i)\n",
    "        #     plt.title(subgraph.edges)\n",
    "        #     i += 1\n",
    "        #     is_check.append(False)\n",
    "        #     print(\n",
    "        #         f\"Non IS found using {solution['solution_method']} on {solution['dataset_name']}\"\n",
    "        #     )\n",
    "        is_check.append(True)\n",
    "\n",
    "\n",
    "    table_row = [dataset_solutions[0]['dataset_name']]\n",
    "\n",
    "    table_row.extend([solution[\"data\"][\"size\"] for solution in dataset_solutions])\n",
    "    table_row.extend([solution[\"time_taken\"] for solution in dataset_solutions])\n",
    "    table_row.extend(is_check)\n",
    "\n",
    "    table_data.append(table_row)\n",
    "\n",
    "table_headers = [\"Dataset Name\"]\n",
    "\n",
    "table_headers.extend([solver[\"name\"] + \" Solution Size\" for solver in solvers])\n",
    "table_headers.extend([solver[\"name\"] + \" Solution Time\" for solver in solvers])\n",
    "table_headers.extend([solver[\"name\"] + \" Solution IS\" for solver in solvers])\n",
    "\n",
    "table = pandas.DataFrame(table_data, columns=table_headers)\n",
    "table\n",
    "table.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Find Exact Sets\n",
    "```{note}\n",
    "Result is indexed by 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_of_interest = \"GNM_n50_m613_seed6\"\n",
    "\n",
    "for dataset_solutions in datasets_solutions:\n",
    "    for solution in dataset_solutions:\n",
    "        if (solution[\"dataset_name\"] == graph_of_interest):\n",
    "            set_solution = []\n",
    "            for idx, x in enumerate(solution[\"data\"][\"graph_mask\"]):\n",
    "                if x == 1:\n",
    "                    set_solution.append(idx)\n",
    "            print(solution[\"solution_method\"],set_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Save Solutions to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read_object(filename):\n",
    "    with open(filename, 'rb') as outp:  # Overwrites any existing file.\n",
    "        return pickle.load(outp)\n",
    "\n",
    "solutions = save_object(solutions, \"solutions.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
